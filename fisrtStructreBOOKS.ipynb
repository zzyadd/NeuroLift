{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching Analysing Quantitative Data with R: Invalid URL 'Analysing Quantitative Data with R': No scheme supplied. Perhaps you meant https://Analysing Quantitative Data with R?\n",
      "Error fetching PO11Q: Seminar Companion: No connection adapters were found for 'PO11Q: Seminar Companion'\n",
      "Error fetching PO3B3: Quantitative Pathway: No connection adapters were found for 'PO3B3: Quantitative Pathway'\n",
      "Error fetching PO12Q: Seminar Companion: No connection adapters were found for 'PO12Q: Seminar Companion'\n",
      "Error fetching PO12Q: Seminar Companion: No connection adapters were found for 'PO12Q: Seminar Companion'\n",
      "Error fetching PO3B3: Quantitative Pathway: No connection adapters were found for 'PO3B3: Quantitative Pathway'\n",
      "Error fetching R Function Flashcards: Invalid URL 'R Function Flashcards': No scheme supplied. Perhaps you meant https://R Function Flashcards?\n",
      "Error fetching : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Extracted 15912 links and saved to N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\firstBooksSummry2.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "from IPython.display import display\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def extract_links_from_page(url):\n",
    "    try:\n",
    "        response = requests.get(url, verify=False)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Selecting the required div\n",
    "        book_summary_div = soup.select_one(\"body > div.book.without-animation.with-summary.font-size-2.font-family-1 > div.book-summary\")\n",
    "        \n",
    "        if not book_summary_div:\n",
    "            print(f\"No matching div found for {url}\")\n",
    "            return []\n",
    "        \n",
    "        # Extracting all href links\n",
    "        links = [a['href'] for a in book_summary_div.find_all('a', href=True)]\n",
    "        return links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    input_file = r\"N:\\CS\\rohana DS\\linksTEXT\\1\\first_structured_books.txt\"\n",
    "    output_file = r\"N:\\CS\\rohana DS\\linksTEXT\\1\\firstBooksSummry2.txt\"\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(\"Error: The specified input file does not exist.\")\n",
    "        return\n",
    "\n",
    "    firstBooksSummry = []\n",
    "    \n",
    "    with open(input_file, \"r\") as file:\n",
    "        links = file.read().splitlines()\n",
    "    \n",
    "    for link in links:\n",
    "        firstBooksSummry.extend(extract_links_from_page(link))\n",
    "    \n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(\"\\n\".join(firstBooksSummry))\n",
    "    \n",
    "    display(f\"Extracted {len(firstBooksSummry)} links and saved to {output_file}\")\n",
    "\n",
    "# Run the main function in Jupyter Notebook\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid links:\n",
      " - \n",
      " - PO12Q: Seminar Companion\n",
      " - R Function Flashcards\n",
      " - PO3B3: Quantitative Pathway\n",
      " - PO11Q: Seminar Companion\n",
      " - Analysing Quantitative Data with R\n",
      " - https://mathstat.slu.edu/~speegle/_book/preface.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Extracted 15692 links and saved to N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\firstBooksSummry.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def is_valid_url(url):\n",
    "    return re.match(r'^(https?:\\/\\/)?([\\da-z.-]+)\\.([a-z.]{2,6})([\\/\\w .-]*)*\\/?$', url)\n",
    "\n",
    "def extract_links_from_page(url):\n",
    "    try:\n",
    "        time.sleep(5)  # Wait for 5 seconds before scraping\n",
    "        response = requests.get(url, verify=False)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Selecting the required div\n",
    "        book_summary_div = soup.select_one(\"body > div.book.without-animation.with-summary.font-size-2.font-family-1 > div.book-summary\")\n",
    "        \n",
    "        if not book_summary_div:\n",
    "            print(f\"No matching div found for {url}\")\n",
    "            return []\n",
    "        \n",
    "        # Extracting all href links\n",
    "        links = [a['href'] for a in book_summary_div.find_all('a', href=True)]\n",
    "        return links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    input_file = r\"N:\\CS\\rohana DS\\linksTEXT\\1\\first_structured_books.txt\"\n",
    "    output_file = r\"N:\\CS\\rohana DS\\linksTEXT\\1\\firstBooksSummry.txt\"\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(\"Error: The specified input file does not exist.\")\n",
    "        return\n",
    "\n",
    "    firstBooksSummry = []\n",
    "    \n",
    "    with open(input_file, \"r\") as file:\n",
    "        links = file.read().splitlines()\n",
    "    \n",
    "    valid_links = [link for link in links if is_valid_url(link)]\n",
    "    invalid_links = set(links) - set(valid_links)\n",
    "    \n",
    "    if invalid_links:\n",
    "        print(\"Skipping invalid links:\")\n",
    "        for link in invalid_links:\n",
    "            print(f\" - {link}\")\n",
    "    \n",
    "    for link in valid_links:\n",
    "        firstBooksSummry.extend(extract_links_from_page(link))\n",
    "    \n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(\"\\n\".join(firstBooksSummry))\n",
    "    \n",
    "    display(f\"Extracted {len(firstBooksSummry)} links and saved to {output_file}\")\n",
    "\n",
    "# Run the main function in Jupyter Notebook\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid links:\n",
      " - \n",
      " - PO12Q: Seminar Companion\n",
      " - R Function Flashcards\n",
      " - PO3B3: Quantitative Pathway\n",
      " - PO11Q: Seminar Companion\n",
      " - Analysing Quantitative Data with R\n",
      " - https://mathstat.slu.edu/~speegle/_book/preface.html\n",
      "Processing: https://brouwern.github.io/lbrb/\n",
      "Found 214 links in https://brouwern.github.io/lbrb/\n",
      "Total collected links so far: 31598\n",
      "Processing: https://edwinth.github.io/ADSwR/\n",
      "Found 62 links in https://edwinth.github.io/ADSwR/\n",
      "Total collected links so far: 31660\n",
      "Processing: https://michael-franke.github.io/intro-data-analysis/index.html\n",
      "Found 304 links in https://michael-franke.github.io/intro-data-analysis/index.html\n",
      "Total collected links so far: 31964\n",
      "Processing: https://bookdown.org/pingapang9/linear_models_bookdown/\n",
      "Found 247 links in https://bookdown.org/pingapang9/linear_models_bookdown/\n",
      "Total collected links so far: 32211\n",
      "Processing: https://statswithr.github.io/book/\n",
      "Found 114 links in https://statswithr.github.io/book/\n",
      "Total collected links so far: 32325\n",
      "Processing: https://paezha.github.io/spatial-analysis-r/\n",
      "Found 303 links in https://paezha.github.io/spatial-analysis-r/\n",
      "Total collected links so far: 32628\n",
      "Processing: https://book.stat420.org/\n",
      "Found 209 links in https://book.stat420.org/\n",
      "Total collected links so far: 32837\n",
      "Processing: https://bookdown.org/michael_bcalles/gis-crash-course-in-r/\n",
      "Found 43 links in https://bookdown.org/michael_bcalles/gis-crash-course-in-r/\n",
      "Total collected links so far: 32880\n",
      "Processing: https://bookdown.org/egarpor/inference/\n",
      "Found 90 links in https://bookdown.org/egarpor/inference/\n",
      "Total collected links so far: 32970\n",
      "Processing: https://www.bayesrulesbook.com/\n",
      "Found 325 links in https://www.bayesrulesbook.com/\n",
      "Total collected links so far: 33295\n",
      "Processing: https://bookdown.org/roback/bookdown-BeyondMLR/\n",
      "Found 287 links in https://bookdown.org/roback/bookdown-BeyondMLR/\n",
      "Total collected links so far: 33582\n",
      "Processing: https://umatter.github.io/BigData/\n",
      "Found 176 links in https://umatter.github.io/BigData/\n",
      "Total collected links so far: 33758\n",
      "Processing: https://textbook.coleridgeinitiative.org/\n",
      "Found 214 links in https://textbook.coleridgeinitiative.org/\n",
      "Total collected links so far: 33972\n",
      "Processing: https://bookdown.org/yihui/bookdown/\n",
      "Found 82 links in https://bookdown.org/yihui/bookdown/\n",
      "Total collected links so far: 34054\n",
      "Processing: https://compgenomr.github.io/book/\n",
      "Found 267 links in https://compgenomr.github.io/book/\n",
      "Total collected links so far: 34321\n",
      "Processing: https://bookdown.org/hneth/ds4psy/\n",
      "Found 636 links in https://bookdown.org/hneth/ds4psy/\n",
      "Total collected links so far: 34957\n",
      "Processing: https://bgweber.github.io/\n",
      "Found 92 links in https://bgweber.github.io/\n",
      "Total collected links so far: 35049\n",
      "Processing: https://livebook.datascienceheroes.com/\n",
      "Found 154 links in https://livebook.datascienceheroes.com/\n",
      "Total collected links so far: 35203\n",
      "Processing: https://datasciencepractice.study/\n",
      "Found 132 links in https://datasciencepractice.study/\n",
      "Total collected links so far: 35335\n",
      "Processing: https://rkabacoff.github.io/datavis/\n",
      "Found 167 links in https://rkabacoff.github.io/datavis/\n",
      "Total collected links so far: 35502\n",
      "Processing: https://datascienceineducation.com/\n",
      "Found 366 links in https://datascienceineducation.com/\n",
      "Total collected links so far: 35868\n",
      "Processing: https://datasciencejuliahackers.com/\n",
      "Found 94 links in https://datasciencejuliahackers.com/\n",
      "Total collected links so far: 35962\n",
      "Processing: https://datasciencebook.ca/\n",
      "Found 256 links in https://datasciencebook.ca/\n",
      "Total collected links so far: 36218\n",
      "Processing: https://datasciencedesign.com/\n",
      "Found 78 links in https://datasciencedesign.com/\n",
      "Total collected links so far: 36296\n",
      "Processing: https://csgillespie.github.io/efficientR/\n",
      "Found 208 links in https://csgillespie.github.io/efficientR/\n",
      "Total collected links so far: 36504\n",
      "Processing: https://ema.drwhy.ai/\n",
      "Found 241 links in https://ema.drwhy.ai/\n",
      "Total collected links so far: 36745\n",
      "Processing: https://bookdown.org/rdpeng/exdata/\n",
      "Found 156 links in https://bookdown.org/rdpeng/exdata/\n",
      "Total collected links so far: 36901\n",
      "Processing: https://okanbulut.github.io/bigdata/\n",
      "Found 78 links in https://okanbulut.github.io/bigdata/\n",
      "Total collected links so far: 36979\n",
      "Processing: https://smithjd.github.io/sql-pet/\n",
      "Found 222 links in https://smithjd.github.io/sql-pet/\n",
      "Total collected links so far: 37201\n",
      "Processing: http://www.feat.engineering/\n",
      "Found 144 links in http://www.feat.engineering/\n",
      "Total collected links so far: 37345\n",
      "Processing: https://bookdown.org/lisakmnsk/LMU_FINTECH_financial_data_science/\n",
      "Found 43 links in https://bookdown.org/lisakmnsk/LMU_FINTECH_financial_data_science/\n",
      "Total collected links so far: 37388\n",
      "Processing: https://clauswilke.com/dataviz/\n",
      "Found 128 links in https://clauswilke.com/dataviz/\n",
      "Total collected links so far: 37516\n",
      "Processing: https://bookdown.org/mcwimberly/gdswr-book/\n",
      "Found 121 links in https://bookdown.org/mcwimberly/gdswr-book/\n",
      "Total collected links so far: 37637\n",
      "Processing: https://handsondataviz.org/\n",
      "Found 138 links in https://handsondataviz.org/\n",
      "Total collected links so far: 37775\n",
      "Processing: https://bradleyboehmke.github.io/HOML/\n",
      "Found 293 links in https://bradleyboehmke.github.io/HOML/\n",
      "Total collected links so far: 38068\n",
      "Processing: https://rstudio-education.github.io/hopr/\n",
      "Found 149 links in https://rstudio-education.github.io/hopr/\n",
      "Total collected links so far: 38217\n",
      "Processing: https://christophm.github.io/interpretable-ml-book/\n",
      "Found 221 links in https://christophm.github.io/interpretable-ml-book/\n",
      "Total collected links so far: 38438\n",
      "Processing: https://mgimond.github.io/Spatial/\n",
      "Found 258 links in https://mgimond.github.io/Spatial/\n",
      "Total collected links so far: 38696\n",
      "Processing: https://bookdown.org/igisc/EnvDataSci/\n",
      "Found 246 links in https://bookdown.org/igisc/EnvDataSci/\n",
      "Total collected links so far: 38942\n",
      "Processing: https://geobgu.xyz/r/\n",
      "Found 341 links in https://geobgu.xyz/r/\n",
      "Total collected links so far: 39283\n",
      "Processing: https://learningstatisticswithr.com/book/\n",
      "Found 579 links in https://learningstatisticswithr.com/book/\n",
      "Total collected links so far: 39862\n",
      "Processing: https://shainarace.github.io/LinearAlgebra/\n",
      "Found 222 links in https://shainarace.github.io/LinearAlgebra/\n",
      "Total collected links so far: 40084\n",
      "Processing: https://bookdown.org/rdpeng/RProgDA/\n",
      "Found 218 links in https://bookdown.org/rdpeng/RProgDA/\n",
      "Total collected links so far: 40302\n",
      "Processing: https://therinspark.com/\n",
      "Found 208 links in https://therinspark.com/\n",
      "Total collected links so far: 40510\n",
      "Processing: https://rkabacoff.github.io/datavis/\n",
      "Found 167 links in https://rkabacoff.github.io/datavis/\n",
      "Total collected links so far: 40677\n",
      "Processing: http://modern-rstats.eu/\n",
      "Found 186 links in http://modern-rstats.eu/\n",
      "Total collected links so far: 40863\n",
      "Processing: https://www.modernstatisticswithr.com/\n",
      "Found 462 links in https://www.modernstatisticswithr.com/\n",
      "Total collected links so far: 41325\n",
      "Processing: https://m-clark.github.io/data-processing-and-visualization/\n",
      "Found 355 links in https://m-clark.github.io/data-processing-and-visualization/\n",
      "Total collected links so far: 41680\n",
      "Processing: https://itsleeds.github.io/QGIS-intro/\n",
      "Found 79 links in https://itsleeds.github.io/QGIS-intro/\n",
      "Total collected links so far: 41759\n",
      "Processing: https://rc2e.com/\n",
      "Found 320 links in https://rc2e.com/\n",
      "Total collected links so far: 42079\n",
      "Processing: https://argoshare.is.ed.ac.uk/healthyr_book/\n",
      "Found 323 links in https://argoshare.is.ed.ac.uk/healthyr_book/\n",
      "Total collected links so far: 42402\n",
      "Processing: https://r-graphics.org/\n",
      "Found 766 links in https://r-graphics.org/\n",
      "Total collected links so far: 43168\n",
      "Processing: https://bookdown.org/rdpeng/rprogdatascience/\n",
      "Found 169 links in https://bookdown.org/rdpeng/rprogdatascience/\n",
      "Total collected links so far: 43337\n",
      "Processing: https://r02pro.github.io/\n",
      "Found 431 links in https://r02pro.github.io/\n",
      "Total collected links so far: 43768\n",
      "Processing: https://mspeekenbrink.github.io/sdam-r-companion/\n",
      "Found 143 links in https://mspeekenbrink.github.io/sdam-r-companion/\n",
      "Total collected links so far: 43911\n",
      "Processing: https://bookdown.org/yihui/rmarkdown/\n",
      "Found 296 links in https://bookdown.org/yihui/rmarkdown/\n",
      "Total collected links so far: 44207\n",
      "Processing: https://moderndive.com/\n",
      "Found 343 links in https://moderndive.com/\n",
      "Total collected links so far: 44550\n",
      "Processing: https://epurdom.github.io/Stat131A/book/\n",
      "Found 156 links in https://epurdom.github.io/Stat131A/book/\n",
      "Total collected links so far: 44706\n",
      "Processing: https://statsthinking21.github.io/statsthinking21-core-site/index.html\n",
      "Found 260 links in https://statsthinking21.github.io/statsthinking21-core-site/index.html\n",
      "Total collected links so far: 44966\n",
      "Processing: https://mspeekenbrink.github.io/sdam-book/\n",
      "Found 236 links in https://mspeekenbrink.github.io/sdam-book/\n",
      "Total collected links so far: 45202\n",
      "Processing: https://stat545.com/\n",
      "Found 442 links in https://stat545.com/\n",
      "Total collected links so far: 45644\n",
      "Processing: https://cengel.github.io/R-text-analysis/\n",
      "Found 22 links in https://cengel.github.io/R-text-analysis/\n",
      "Total collected links so far: 45666\n",
      "Processing: https://bookdown.org/f_lennert/text-mining-book/\n",
      "Found 91 links in https://bookdown.org/f_lennert/text-mining-book/\n",
      "Total collected links so far: 45757\n",
      "Processing: https://ohdsi.github.io/TheBookOfOhdsi/\n",
      "Found 439 links in https://ohdsi.github.io/TheBookOfOhdsi/\n",
      "Total collected links so far: 46196\n",
      "Processing: https://data-cleaning.github.io/validate/\n",
      "Found 64 links in https://data-cleaning.github.io/validate/\n",
      "Total collected links so far: 46260\n",
      "Processing: https://drfloreiche.github.io/AQDR/\n",
      "Found 115 links in https://drfloreiche.github.io/AQDR/\n",
      "Total collected links so far: 46375\n",
      "Processing: https://drfloreiche.github.io/PO11Q/\n",
      "Found 128 links in https://drfloreiche.github.io/PO11Q/\n",
      "Total collected links so far: 46503\n",
      "Processing: https://drfloreiche.github.io/PO3B3/\n",
      "Found 99 links in https://drfloreiche.github.io/PO3B3/\n",
      "Total collected links so far: 46602\n",
      "Processing: https://drfloreiche.github.io/PO12Q/\n",
      "Found 139 links in https://drfloreiche.github.io/PO12Q/\n",
      "Total collected links so far: 46741\n",
      "Processing: https://drfloreiche.github.io/PO12Q/\n",
      "Found 139 links in https://drfloreiche.github.io/PO12Q/\n",
      "Total collected links so far: 46880\n",
      "Processing: https://drfloreiche.github.io/PO3B3/\n",
      "Found 99 links in https://drfloreiche.github.io/PO3B3/\n",
      "Total collected links so far: 46979\n",
      "Processing: https://drfloreiche.github.io/POQFlashcards/\n",
      "Found 97 links in https://drfloreiche.github.io/POQFlashcards/\n",
      "Total collected links so far: 47076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Extracted 47076 links and saved to N:\\\\\\\\CS\\\\\\\\rohana DS\\\\\\\\linksTEXT\\\\\\\\1\\\\\\\\firstBooksSummry.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "from IPython.display import display\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def is_valid_url(url):\n",
    "    return re.match(r'^(https?:\\/\\/)?([\\da-z.-]+)\\.([a-z.]{2,6})([\\/\\w .-]*)*\\/?$', url)\n",
    "\n",
    "def extract_links_from_page(url):\n",
    "    try:\n",
    "        print(f\"Processing: {url}\")\n",
    "        time.sleep(5)  # Wait for 5 seconds before scraping\n",
    "        response = requests.get(url, verify=False)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Selecting the required div\n",
    "        book_summary_div = soup.select_one(\"body > div.book.without-animation.with-summary.font-size-2.font-family-1 > div.book-summary\")\n",
    "        \n",
    "        if not book_summary_div:\n",
    "            print(f\"No matching div found for {url}\")\n",
    "            return []\n",
    "        \n",
    "        # Extracting all href links and making them absolute\n",
    "        links = [urljoin(url, a['href']) for a in book_summary_div.find_all('a', href=True)]\n",
    "        print(f\"Found {len(links)} links in {url}\")\n",
    "        return links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    input_file = r\"N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\first_structured_books.txt\"\n",
    "    output_file = r\"N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\firstBooksSummry.txt\"\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(\"Error: The specified input file does not exist.\")\n",
    "        return\n",
    "\n",
    "    firstBooksSummry = []\n",
    "    \n",
    "    # Load existing links if the output file already exists\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\") as file:\n",
    "            firstBooksSummry = file.read().splitlines()\n",
    "    \n",
    "    with open(input_file, \"r\") as file:\n",
    "        links = file.read().splitlines()\n",
    "    \n",
    "    valid_links = [link for link in links if is_valid_url(link)]\n",
    "    invalid_links = set(links) - set(valid_links)\n",
    "    \n",
    "    if invalid_links:\n",
    "        print(\"Skipping invalid links:\")\n",
    "        for link in invalid_links:\n",
    "            print(f\" - {link}\")\n",
    "    \n",
    "    for link in valid_links:\n",
    "        extracted_links = extract_links_from_page(link)\n",
    "        firstBooksSummry.extend(extracted_links)\n",
    "        print(f\"Total collected links so far: {len(firstBooksSummry)}\")\n",
    "    \n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(\"\\n\".join(firstBooksSummry))\n",
    "    \n",
    "    display(f\"Extracted {len(firstBooksSummry)} links and saved to {output_file}\")\n",
    "\n",
    "# Run the main function in Jupyter Notebook\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid links:\n",
      " - \n",
      " - PO12Q: Seminar Companion\n",
      " - R Function Flashcards\n",
      " - PO3B3: Quantitative Pathway\n",
      " - PO11Q: Seminar Companion\n",
      " - Analysing Quantitative Data with R\n",
      " - https://mathstat.slu.edu/~speegle/_book/preface.html\n",
      "Processing: https://brouwern.github.io/lbrb/\n",
      "Found 214 links in https://brouwern.github.io/lbrb/\n",
      "Total collected links so far: 214\n",
      "Processing: https://edwinth.github.io/ADSwR/\n",
      "Found 62 links in https://edwinth.github.io/ADSwR/\n",
      "Total collected links so far: 276\n",
      "Processing: https://michael-franke.github.io/intro-data-analysis/index.html\n",
      "Found 304 links in https://michael-franke.github.io/intro-data-analysis/index.html\n",
      "Total collected links so far: 580\n",
      "Processing: https://bookdown.org/pingapang9/linear_models_bookdown/\n",
      "Found 247 links in https://bookdown.org/pingapang9/linear_models_bookdown/\n",
      "Total collected links so far: 827\n",
      "Processing: https://statswithr.github.io/book/\n",
      "Found 114 links in https://statswithr.github.io/book/\n",
      "Total collected links so far: 941\n",
      "Processing: https://paezha.github.io/spatial-analysis-r/\n",
      "Found 303 links in https://paezha.github.io/spatial-analysis-r/\n",
      "Total collected links so far: 1244\n",
      "Processing: https://book.stat420.org/\n",
      "Found 209 links in https://book.stat420.org/\n",
      "Total collected links so far: 1453\n",
      "Processing: https://bookdown.org/michael_bcalles/gis-crash-course-in-r/\n",
      "Found 43 links in https://bookdown.org/michael_bcalles/gis-crash-course-in-r/\n",
      "Total collected links so far: 1496\n",
      "Processing: https://bookdown.org/egarpor/inference/\n",
      "Found 90 links in https://bookdown.org/egarpor/inference/\n",
      "Total collected links so far: 1586\n",
      "Processing: https://www.bayesrulesbook.com/\n",
      "Found 325 links in https://www.bayesrulesbook.com/\n",
      "Total collected links so far: 1911\n",
      "Processing: https://bookdown.org/roback/bookdown-BeyondMLR/\n",
      "Found 287 links in https://bookdown.org/roback/bookdown-BeyondMLR/\n",
      "Total collected links so far: 2198\n",
      "Processing: https://umatter.github.io/BigData/\n",
      "Found 176 links in https://umatter.github.io/BigData/\n",
      "Total collected links so far: 2374\n",
      "Processing: https://textbook.coleridgeinitiative.org/\n",
      "Found 214 links in https://textbook.coleridgeinitiative.org/\n",
      "Total collected links so far: 2588\n",
      "Processing: https://bookdown.org/yihui/bookdown/\n",
      "Found 82 links in https://bookdown.org/yihui/bookdown/\n",
      "Total collected links so far: 2670\n",
      "Processing: https://compgenomr.github.io/book/\n",
      "Found 267 links in https://compgenomr.github.io/book/\n",
      "Total collected links so far: 2937\n",
      "Processing: https://bookdown.org/hneth/ds4psy/\n",
      "Found 636 links in https://bookdown.org/hneth/ds4psy/\n",
      "Total collected links so far: 3573\n",
      "Processing: https://bgweber.github.io/\n",
      "Found 92 links in https://bgweber.github.io/\n",
      "Total collected links so far: 3665\n",
      "Processing: https://livebook.datascienceheroes.com/\n",
      "Found 154 links in https://livebook.datascienceheroes.com/\n",
      "Total collected links so far: 3819\n",
      "Processing: https://datasciencepractice.study/\n",
      "Found 132 links in https://datasciencepractice.study/\n",
      "Total collected links so far: 3951\n",
      "Processing: https://rkabacoff.github.io/datavis/\n",
      "Found 167 links in https://rkabacoff.github.io/datavis/\n",
      "Total collected links so far: 4118\n",
      "Processing: https://datascienceineducation.com/\n",
      "Found 366 links in https://datascienceineducation.com/\n",
      "Total collected links so far: 4484\n",
      "Processing: https://datasciencejuliahackers.com/\n",
      "Found 94 links in https://datasciencejuliahackers.com/\n",
      "Total collected links so far: 4578\n",
      "Processing: https://datasciencebook.ca/\n",
      "Found 256 links in https://datasciencebook.ca/\n",
      "Total collected links so far: 4834\n",
      "Processing: https://datasciencedesign.com/\n",
      "Found 78 links in https://datasciencedesign.com/\n",
      "Total collected links so far: 4912\n",
      "Processing: https://csgillespie.github.io/efficientR/\n",
      "Found 208 links in https://csgillespie.github.io/efficientR/\n",
      "Total collected links so far: 5120\n",
      "Processing: https://ema.drwhy.ai/\n",
      "Found 241 links in https://ema.drwhy.ai/\n",
      "Total collected links so far: 5361\n",
      "Processing: https://bookdown.org/rdpeng/exdata/\n",
      "Found 156 links in https://bookdown.org/rdpeng/exdata/\n",
      "Total collected links so far: 5517\n",
      "Processing: https://okanbulut.github.io/bigdata/\n",
      "Found 78 links in https://okanbulut.github.io/bigdata/\n",
      "Total collected links so far: 5595\n",
      "Processing: https://smithjd.github.io/sql-pet/\n",
      "Found 222 links in https://smithjd.github.io/sql-pet/\n",
      "Total collected links so far: 5817\n",
      "Processing: http://www.feat.engineering/\n",
      "Found 144 links in http://www.feat.engineering/\n",
      "Total collected links so far: 5961\n",
      "Processing: https://bookdown.org/lisakmnsk/LMU_FINTECH_financial_data_science/\n",
      "Found 43 links in https://bookdown.org/lisakmnsk/LMU_FINTECH_financial_data_science/\n",
      "Total collected links so far: 6004\n",
      "Processing: https://clauswilke.com/dataviz/\n",
      "Found 128 links in https://clauswilke.com/dataviz/\n",
      "Total collected links so far: 6132\n",
      "Processing: https://bookdown.org/mcwimberly/gdswr-book/\n",
      "Found 121 links in https://bookdown.org/mcwimberly/gdswr-book/\n",
      "Total collected links so far: 6253\n",
      "Processing: https://handsondataviz.org/\n",
      "Found 138 links in https://handsondataviz.org/\n",
      "Total collected links so far: 6391\n",
      "Processing: https://bradleyboehmke.github.io/HOML/\n",
      "Found 293 links in https://bradleyboehmke.github.io/HOML/\n",
      "Total collected links so far: 6684\n",
      "Processing: https://rstudio-education.github.io/hopr/\n",
      "Found 149 links in https://rstudio-education.github.io/hopr/\n",
      "Total collected links so far: 6833\n",
      "Processing: https://christophm.github.io/interpretable-ml-book/\n",
      "Found 221 links in https://christophm.github.io/interpretable-ml-book/\n",
      "Total collected links so far: 7054\n",
      "Processing: https://mgimond.github.io/Spatial/\n",
      "Found 258 links in https://mgimond.github.io/Spatial/\n",
      "Total collected links so far: 7312\n",
      "Processing: https://bookdown.org/igisc/EnvDataSci/\n",
      "Found 246 links in https://bookdown.org/igisc/EnvDataSci/\n",
      "Total collected links so far: 7558\n",
      "Processing: https://geobgu.xyz/r/\n",
      "Found 341 links in https://geobgu.xyz/r/\n",
      "Total collected links so far: 7899\n",
      "Processing: https://learningstatisticswithr.com/book/\n",
      "Found 579 links in https://learningstatisticswithr.com/book/\n",
      "Total collected links so far: 8478\n",
      "Processing: https://shainarace.github.io/LinearAlgebra/\n",
      "Found 222 links in https://shainarace.github.io/LinearAlgebra/\n",
      "Total collected links so far: 8700\n",
      "Processing: https://bookdown.org/rdpeng/RProgDA/\n",
      "Found 218 links in https://bookdown.org/rdpeng/RProgDA/\n",
      "Total collected links so far: 8918\n",
      "Processing: https://therinspark.com/\n",
      "Found 208 links in https://therinspark.com/\n",
      "Total collected links so far: 9126\n",
      "Processing: https://rkabacoff.github.io/datavis/\n",
      "Found 167 links in https://rkabacoff.github.io/datavis/\n",
      "Total collected links so far: 9293\n",
      "Processing: http://modern-rstats.eu/\n",
      "Found 186 links in http://modern-rstats.eu/\n",
      "Total collected links so far: 9479\n",
      "Processing: https://www.modernstatisticswithr.com/\n",
      "Found 462 links in https://www.modernstatisticswithr.com/\n",
      "Total collected links so far: 9941\n",
      "Processing: https://m-clark.github.io/data-processing-and-visualization/\n",
      "Found 355 links in https://m-clark.github.io/data-processing-and-visualization/\n",
      "Total collected links so far: 10296\n",
      "Processing: https://itsleeds.github.io/QGIS-intro/\n",
      "Found 79 links in https://itsleeds.github.io/QGIS-intro/\n",
      "Total collected links so far: 10375\n",
      "Processing: https://rc2e.com/\n",
      "Found 320 links in https://rc2e.com/\n",
      "Total collected links so far: 10695\n",
      "Processing: https://argoshare.is.ed.ac.uk/healthyr_book/\n",
      "Found 323 links in https://argoshare.is.ed.ac.uk/healthyr_book/\n",
      "Total collected links so far: 11018\n",
      "Processing: https://r-graphics.org/\n",
      "Found 766 links in https://r-graphics.org/\n",
      "Total collected links so far: 11784\n",
      "Processing: https://bookdown.org/rdpeng/rprogdatascience/\n",
      "Found 169 links in https://bookdown.org/rdpeng/rprogdatascience/\n",
      "Total collected links so far: 11953\n",
      "Processing: https://r02pro.github.io/\n",
      "Found 431 links in https://r02pro.github.io/\n",
      "Total collected links so far: 12384\n",
      "Processing: https://mspeekenbrink.github.io/sdam-r-companion/\n",
      "Found 143 links in https://mspeekenbrink.github.io/sdam-r-companion/\n",
      "Total collected links so far: 12527\n",
      "Processing: https://bookdown.org/yihui/rmarkdown/\n",
      "Found 296 links in https://bookdown.org/yihui/rmarkdown/\n",
      "Total collected links so far: 12823\n",
      "Processing: https://moderndive.com/\n",
      "Found 343 links in https://moderndive.com/\n",
      "Total collected links so far: 13166\n",
      "Processing: https://epurdom.github.io/Stat131A/book/\n",
      "Found 156 links in https://epurdom.github.io/Stat131A/book/\n",
      "Total collected links so far: 13322\n",
      "Processing: https://statsthinking21.github.io/statsthinking21-core-site/index.html\n",
      "Found 260 links in https://statsthinking21.github.io/statsthinking21-core-site/index.html\n",
      "Total collected links so far: 13582\n",
      "Processing: https://mspeekenbrink.github.io/sdam-book/\n",
      "Found 236 links in https://mspeekenbrink.github.io/sdam-book/\n",
      "Total collected links so far: 13818\n",
      "Processing: https://stat545.com/\n",
      "Found 442 links in https://stat545.com/\n",
      "Total collected links so far: 14260\n",
      "Processing: https://cengel.github.io/R-text-analysis/\n",
      "Found 22 links in https://cengel.github.io/R-text-analysis/\n",
      "Total collected links so far: 14282\n",
      "Processing: https://bookdown.org/f_lennert/text-mining-book/\n",
      "Found 91 links in https://bookdown.org/f_lennert/text-mining-book/\n",
      "Total collected links so far: 14373\n",
      "Processing: https://ohdsi.github.io/TheBookOfOhdsi/\n",
      "Found 439 links in https://ohdsi.github.io/TheBookOfOhdsi/\n",
      "Total collected links so far: 14812\n",
      "Processing: https://data-cleaning.github.io/validate/\n",
      "Found 64 links in https://data-cleaning.github.io/validate/\n",
      "Total collected links so far: 14876\n",
      "Processing: https://drfloreiche.github.io/AQDR/\n",
      "Found 115 links in https://drfloreiche.github.io/AQDR/\n",
      "Total collected links so far: 14991\n",
      "Processing: https://drfloreiche.github.io/PO11Q/\n",
      "Found 128 links in https://drfloreiche.github.io/PO11Q/\n",
      "Total collected links so far: 15119\n",
      "Processing: https://drfloreiche.github.io/PO3B3/\n",
      "Found 99 links in https://drfloreiche.github.io/PO3B3/\n",
      "Total collected links so far: 15218\n",
      "Processing: https://drfloreiche.github.io/PO12Q/\n",
      "Found 139 links in https://drfloreiche.github.io/PO12Q/\n",
      "Total collected links so far: 15357\n",
      "Processing: https://drfloreiche.github.io/PO12Q/\n",
      "Found 139 links in https://drfloreiche.github.io/PO12Q/\n",
      "Total collected links so far: 15496\n",
      "Processing: https://drfloreiche.github.io/PO3B3/\n",
      "Found 99 links in https://drfloreiche.github.io/PO3B3/\n",
      "Total collected links so far: 15595\n",
      "Processing: https://drfloreiche.github.io/POQFlashcards/\n",
      "Found 97 links in https://drfloreiche.github.io/POQFlashcards/\n",
      "Total collected links so far: 15692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Extracted 15692 links and saved to N:\\\\\\\\CS\\\\\\\\rohana DS\\\\\\\\linksTEXT\\\\\\\\1\\\\\\\\firstBooksSummry33.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "from IPython.display import display\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def is_valid_url(url):\n",
    "    return re.match(r'^(https?:\\/\\/)?([\\da-z.-]+)\\.([a-z.]{2,6})([\\/\\w .-]*)*\\/?$', url)\n",
    "\n",
    "def extract_links_from_page(url):\n",
    "    try:\n",
    "        print(f\"Processing: {url}\")\n",
    "        time.sleep(5)  # Wait for 5 seconds before scraping\n",
    "        response = requests.get(url, verify=False)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Selecting the required div\n",
    "        book_summary_div = soup.select_one(\"body > div.book.without-animation.with-summary.font-size-2.font-family-1 > div.book-summary\")\n",
    "        \n",
    "        if not book_summary_div:\n",
    "            print(f\"No matching div found for {url}\")\n",
    "            return []\n",
    "        \n",
    "        # Extracting all href links and making them absolute\n",
    "        links = [urljoin(url, a['href']) for a in book_summary_div.find_all('a', href=True)]\n",
    "        print(f\"Found {len(links)} links in {url}\")\n",
    "        return links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    input_file = r\"N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\first_structured_books.txt\"\n",
    "    output_file = r\"N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\firstBooksSummry33.txt\"\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(\"Error: The specified input file does not exist.\")\n",
    "        return\n",
    "\n",
    "    firstBooksSummry33 = []\n",
    "    \n",
    "    with open(input_file, \"r\") as file:\n",
    "        base_links = file.read().splitlines()\n",
    "    \n",
    "    valid_links = [link for link in base_links if is_valid_url(link)]\n",
    "    invalid_links = set(base_links) - set(valid_links)\n",
    "    \n",
    "    if invalid_links:\n",
    "        print(\"Skipping invalid links:\")\n",
    "        for link in invalid_links:\n",
    "            print(f\" - {link}\")\n",
    "    \n",
    "    for base_link in valid_links:\n",
    "        extracted_links = extract_links_from_page(base_link)\n",
    "        full_links = [urljoin(base_link, link) for link in extracted_links]\n",
    "        firstBooksSummry33.extend(full_links)\n",
    "        print(f\"Total collected links so far: {len(firstBooksSummry33)}\")\n",
    "    \n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(\"\\n\".join(firstBooksSummry33))\n",
    "    \n",
    "    display(f\"Extracted {len(firstBooksSummry33)} links and saved to {output_file}\")\n",
    "\n",
    "# Run the main function in Jupyter Notebook\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching text from: https://brouwern.github.io/lbrb/\n",
      "Saved text from https://brouwern.github.io/lbrb/ to N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\ExtractedTexts\\https___brouwern_github_io_lbrb_.txt\n",
      "Fetching text from: https://brouwern.github.io/lbrb/index.html\n",
      "Saved text from https://brouwern.github.io/lbrb/index.html to N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\ExtractedTexts\\https___brouwern_github_io_lbrb_index_html.txt\n",
      "Fetching text from: https://brouwern.github.io/lbrb/downloadR.html\n",
      "Saved text from https://brouwern.github.io/lbrb/downloadR.html to N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\ExtractedTexts\\https___brouwern_github_io_lbrb_downloadR_html.txt\n",
      "Fetching text from: https://brouwern.github.io/lbrb/downloadR.html#preface\n",
      "Saved text from https://brouwern.github.io/lbrb/downloadR.html#preface to N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\ExtractedTexts\\https___brouwern_github_io_lbrb_downloadR_html_preface.txt\n",
      "Fetching text from: https://brouwern.github.io/lbrb/downloadR.html#introduction-to-r\n",
      "Saved text from https://brouwern.github.io/lbrb/downloadR.html#introduction-to-r to N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\ExtractedTexts\\https___brouwern_github_io_lbrb_downloadR_html_introduction_to_r.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Text extraction completed.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def extract_text_from_page(url, output_dir):\n",
    "    try:\n",
    "        print(f\"Fetching text from: {url}\")\n",
    "        time.sleep(5)  # Wait for 5 seconds before scraping\n",
    "        response = requests.get(url, verify=False)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Selecting the required div\n",
    "        book_body_div = soup.select_one(\"body > div > div.book-body\")\n",
    "        \n",
    "        if not book_body_div:\n",
    "            print(f\"No matching text found for {url}\")\n",
    "            return None\n",
    "        \n",
    "        text_content = book_body_div.get_text(strip=True)\n",
    "        \n",
    "        # Save text to file\n",
    "        filename = os.path.join(output_dir, f\"{re.sub(r'[^a-zA-Z0-9]', '_', url)}.txt\")\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(text_content)\n",
    "        \n",
    "        print(f\"Saved text from {url} to {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "def main():\n",
    "    input_file = r\"N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\firstSS.txt\"\n",
    "    text_output_dir = r\"N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\ExtractedTexts\"\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(\"Error: The specified input file does not exist.\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(text_output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(input_file, \"r\") as file:\n",
    "        links = file.read().splitlines()\n",
    "    \n",
    "    for link in links:\n",
    "        extract_text_from_page(link, text_output_dir)\n",
    "    \n",
    "    display(\"Text extraction completed.\")\n",
    "\n",
    "# Run the main function in Jupyter Notebook\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 81 PDF links and saved to N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\editFirstStructure.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def generate_pdf_links(input_file, output_file):\n",
    "    if not os.path.exists(input_file):\n",
    "        print(\"Error: The specified input file does not exist.\")\n",
    "        return\n",
    "    \n",
    "    pdf_links = []\n",
    "    with open(input_file, \"r\") as file:\n",
    "        links = file.read().splitlines()\n",
    "    \n",
    "    for link in links:\n",
    "        parts = link.rstrip('/').split(\"/\")\n",
    "        if parts:\n",
    "            pdf_link = link.rstrip('/') + \"/\" + parts[-1] + \".pdf\"\n",
    "            pdf_links.append(pdf_link)\n",
    "    \n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(\"\\n\".join(pdf_links))\n",
    "    \n",
    "    print(f\"Generated {len(pdf_links)} PDF links and saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_file = r\"N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\first_structured_books.txt\"\n",
    "    output_file = r\"N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\editFirstStructure.txt\"\n",
    "    \n",
    "    generate_pdf_links(input_file, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: https://brouwern.github.io/lbrb/\n",
      "⚠️ SSL Error for https://brouwern.github.io/lbrb/: HTTPSConnectionPool(host='brouwern.github.io', port=443): Max retries exceeded with url: /lbrb/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1002)')))\n",
      "Checking: https://edwinth.github.io/ADSwR/\n",
      "⚠️ SSL Error for https://edwinth.github.io/ADSwR/: HTTPSConnectionPool(host='edwinth.github.io', port=443): Max retries exceeded with url: /ADSwR/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1002)')))\n",
      "Checking: https://michael-franke.github.io/intro-data-analysis/index.html\n",
      "⚠️ SSL Error for https://michael-franke.github.io/intro-data-analysis/index.html: HTTPSConnectionPool(host='michael-franke.github.io', port=443): Max retries exceeded with url: /intro-data-analysis/index.html (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1002)')))\n",
      "Checking: https://bookdown.org/pingapang9/linear_models_bookdown/\n",
      "⚠️ SSL Error for https://bookdown.org/pingapang9/linear_models_bookdown/: HTTPSConnectionPool(host='bookdown.org', port=443): Max retries exceeded with url: /pingapang9/linear_models_bookdown/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1002)')))\n",
      "Checking: https://statswithr.github.io/book/\n",
      "⚠️ SSL Error for https://statswithr.github.io/book/: HTTPSConnectionPool(host='statswithr.github.io', port=443): Max retries exceeded with url: /book/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1002)')))\n",
      "Checking: https://paezha.github.io/spatial-analysis-r/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Wait for 3 seconds before making the request\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Fetch HTML content with SSL verification disabled\u001b[39;00m\n\u001b[0;32m     36\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verify\u001b[38;5;241m=\u001b[39mcertifi\u001b[38;5;241m.\u001b[39mwhere())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import wget\n",
    "import urllib3\n",
    "import certifi\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Input file containing book links\n",
    "input_file = r\"N:\\\\CS\\\\rohana DS\\\\linksTEXT\\\\1\\\\first_structured_books.txt\"\n",
    "\n",
    "# Output folder for downloaded PDFs\n",
    "output_folder = r\"N:\\\\CS\\\\rohana DS\\\\downloaded_pdfs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# File to store successfully downloaded links\n",
    "downloaded_links_file = os.path.join(output_folder, \"downloaded_links.txt\")\n",
    "\n",
    "# Read links from the text file\n",
    "with open(input_file, \"r\") as file:\n",
    "    links = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# Open the downloaded links file in append mode\n",
    "with open(downloaded_links_file, \"a\") as log_file:\n",
    "    for index, url in enumerate(links, start=1):\n",
    "        try:\n",
    "            print(f\"Checking: {url}\")\n",
    "\n",
    "            # Wait for 3 seconds before making the request\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Fetch HTML content with SSL verification disabled\n",
    "            response = requests.get(url, timeout=10, verify=certifi.where())\n",
    "            response.raise_for_status()  # Raise error for bad status codes\n",
    "\n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract PDF link if available\n",
    "            pdf_element = soup.select_one('.dropdown-menu .buttons a')\n",
    "\n",
    "            if pdf_element and 'href' in pdf_element.attrs:\n",
    "                pdf_url = pdf_element['href']\n",
    "                \n",
    "                # Handle relative URLs\n",
    "                if not pdf_url.startswith(\"http\"):\n",
    "                    pdf_url = requests.compat.urljoin(url, pdf_url)\n",
    "\n",
    "                print(f\"Found PDF: {pdf_url}\")\n",
    "\n",
    "                # Generate a filename\n",
    "                pdf_filename = os.path.join(output_folder, f\"book_{index}.pdf\")\n",
    "\n",
    "                # Download the PDF\n",
    "                wget.download(pdf_url, pdf_filename)\n",
    "                print(f\"\\nDownloaded: {pdf_filename}\")\n",
    "\n",
    "                # Log the successful download\n",
    "                log_file.write(f\"{url}\\n\")\n",
    "                log_file.flush()  # Ensure data is written immediately\n",
    "\n",
    "            else:\n",
    "                print(\"❌ No PDF found.\")\n",
    "\n",
    "        except requests.exceptions.SSLError as ssl_err:\n",
    "            print(f\"⚠️ SSL Error for {url}: {ssl_err}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as req_err:\n",
    "            print(f\"⚠️ Error fetching {url}: {req_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\zy565\\anaconda3\\lib\\site-packages (2.29.0)\n",
      "Requirement already satisfied: wget in c:\\users\\zy565\\anaconda3\\lib\\site-packages (3.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\zy565\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\zy565\\anaconda3\\lib\\site-packages (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zy565\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zy565\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zy565\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\zy565\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests wget beautifulsoup4 certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
