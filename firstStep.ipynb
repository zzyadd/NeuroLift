{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Linkes For Each Book in DatLinks Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Folder containing files with links\n",
    "FOLDER_PATH = \"/mnt/n/CS/rohana DS/Book from DATLinux\"\n",
    "\n",
    "# üìå Folder to save extracted text files\n",
    "OUTPUT_FOLDER = \"/mnt/c/Users/Zy565/Downloads/m4ro3_t5rg/dataForProject\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    \"\"\"Fetches the webpage, waits 30 seconds, then extracts text.\"\"\"\n",
    "    print(f\"üîó Opening: {url}\")\n",
    "    time.sleep(30)  # Wait for 30 seconds before scraping\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        return soup.get_text(separator=\"\\n\", strip=True)\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_folder():\n",
    "    \"\"\"Extracts links from HTML files and scrapes the text.\"\"\"\n",
    "    for file in os.listdir(FOLDER_PATH):\n",
    "        if file.endswith(\".html\") or file.endswith(\".htm\"):\n",
    "            file_path = os.path.join(FOLDER_PATH, file)\n",
    "            book_name = os.path.splitext(file)[0]  # Remove .html extension\n",
    "            \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "            # Extract the first link (modify if multiple links exist)\n",
    "            link_tag = soup.find(\"a\", href=True)\n",
    "            if link_tag:\n",
    "                url = link_tag[\"href\"]\n",
    "                text = extract_text_from_url(url)\n",
    "\n",
    "                # Save the extracted text\n",
    "                output_file = os.path.join(OUTPUT_FOLDER, f\"{book_name}.txt\")\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(text)\n",
    "\n",
    "                print(f\"‚úÖ Saved: {output_file}\")\n",
    "            else:\n",
    "                print(f\"‚ö† No link found in {file}\")\n",
    "                \n",
    "                \n",
    "process_folder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
