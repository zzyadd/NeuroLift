{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://d2l.ai/\n",
      "No matching element found for selector 'body > div > div > header.mdl-layout__drawer' in https://d2l.ai/\n",
      "Extracted 0 links and saved to N:\\CS\\rohana DS\\linksTEXT\\12\\12_BooksSummry.txt\n",
      "Completed processing for folder #12\n",
      "Pipeline execution completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import urllib3\n",
    "import re\n",
    "import hashlib\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Constants\n",
    "BASE_DIR = r\"N:\\CS\\rohana DS\\linksTEXT\"\n",
    "FOLDERS = range(12, 13)  # From 7 to 16\n",
    "MAX_THREADS = 5  # Number of concurrent threads\n",
    "DELAY = 6  # Delay between requests to avoid being blocked\n",
    "\n",
    "def is_valid_url(url):\n",
    "    return True\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"Reads the content of a file if it exists, otherwise returns None.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read().strip()\n",
    "    print(f\"Warning: {file_path} not found.\")\n",
    "    return None\n",
    "\n",
    "def extract_links_from_page(url, selector):\n",
    "    \"\"\"Extracts href links from a webpage based on the given selector.\"\"\"\n",
    "    try:\n",
    "        print(f\"Processing: {url}\")\n",
    "        time.sleep(DELAY)  # Delay to avoid request throttling\n",
    "        response = requests.get(url, verify=False, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        target_element = soup.select_one(selector)\n",
    "        if not target_element:\n",
    "            print(f\"No matching element found for selector '{selector}' in {url}\")\n",
    "            return []\n",
    "\n",
    "        links = [urljoin(url, a['href']) for a in target_element.find_all('a', href=True)]\n",
    "        print(f\"Found {len(links)} links in {url}\")\n",
    "        return links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_text_from_page(url, selector, text_output_dir):\n",
    "    \"\"\"Extracts text content from a webpage and saves it as a file.\"\"\"\n",
    "    try:\n",
    "        print(f\"Fetching text from: {url}\")\n",
    "        time.sleep(DELAY - 1)  # Delay for politeness\n",
    "        response = requests.get(url, verify=False, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        target_element = soup.select_one(selector)\n",
    "        if not target_element:\n",
    "            print(f\"No matching text found for selector '{selector}' in {url}\")\n",
    "            return\n",
    "        \n",
    "        text_content = target_element.get_text(strip=True)\n",
    "        filename = os.path.join(text_output_dir, f\"{hashlib.md5(url.encode()).hexdigest()}.txt\")\n",
    "        \n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(text_content)\n",
    "        \n",
    "        print(f\"Saved text from {url} to {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "def process_folder(folder_num):\n",
    "    \"\"\"Processes each folder, extracts links, and scrapes text.\"\"\"\n",
    "    folder_path = os.path.join(BASE_DIR, f\"{folder_num}\")\n",
    "    \n",
    "    # Define file paths\n",
    "    structured_books_file = os.path.join(folder_path, f\"{folder_num}_structured_books.txt\")\n",
    "    summary_structure_file = os.path.join(folder_path, f\"{folder_num}_summary_structure.txt\")\n",
    "    books_summary_file = os.path.join(folder_path, f\"{folder_num}_BooksSummry.txt\")\n",
    "    body_text_file = os.path.join(folder_path, f\"{folder_num}_body_text.txt\")\n",
    "    text_output_dir = os.path.join(folder_path, \"ExtractedTexts\")\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(text_output_dir, exist_ok=True)\n",
    "\n",
    "    # Read base links\n",
    "    base_links = read_file(structured_books_file)\n",
    "    if not base_links:\n",
    "        return\n",
    "    base_links = base_links.splitlines()\n",
    "\n",
    "    # Read selectors\n",
    "    summary_selector = read_file(summary_structure_file)\n",
    "    body_selector = read_file(body_text_file)\n",
    "    if not summary_selector or not body_selector:\n",
    "        return\n",
    "\n",
    "    valid_links = [link for link in base_links if is_valid_url(link)]\n",
    "    if not valid_links:\n",
    "        print(f\"No valid links found in {structured_books_file}\")\n",
    "        return\n",
    "\n",
    "    # Extracting all links\n",
    "    extracted_links = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "        results = executor.map(lambda url: extract_links_from_page(url, summary_selector), valid_links)\n",
    "        for result in results:\n",
    "            extracted_links.extend(result)\n",
    "\n",
    "    # Save extracted links\n",
    "    with open(books_summary_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(extracted_links))\n",
    "    print(f\"Extracted {len(extracted_links)} links and saved to {books_summary_file}\")\n",
    "\n",
    "    # Extract text from extracted links\n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "        executor.map(lambda url: extract_text_from_page(url, body_selector, text_output_dir), extracted_links)\n",
    "\n",
    "    print(f\"Completed processing for folder #{folder_num}\")\n",
    "\n",
    "def main():\n",
    "    for folder_num in FOLDERS:\n",
    "        process_folder(folder_num)\n",
    "\n",
    "    print(\"Pipeline execution completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
